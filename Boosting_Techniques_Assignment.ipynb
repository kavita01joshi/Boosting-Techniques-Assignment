{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners?**\n",
        "\n",
        "* Boosting is a method that builds a series of models, each trying to correct the errors made by the previous one. It’s especially useful when individual models (like shallow decision trees) perform only slightly better than random guessing.\n",
        "- Weak learner: A model that performs just above chance level.\n",
        "- Strong learner: A model with high accuracy and generalization ability.\n",
        "\n",
        "**how it improves weak learners-**\n",
        "\n",
        "Boosting improves weak learners through sequential training and error correction:\n",
        "- Initial Model: Train a weak learner on the dataset.\n",
        "- Error Focus: Identify where the model made mistakes.\n",
        "- Weight Adjustment: Increase the importance (weights) of misclassified examples.\n",
        "- Next Model: Train a new model that focuses more on these hard examples.\n",
        "- Repeat: Continue this process for a set number of iterations or until performance stabilizes.\n",
        "- Final Prediction: Combine all models (usually via weighted voting or averaging) to make the final decision.\n",
        "This process reduces both bias and variance, leading to better performance on complex tasks.\n",
        "\n",
        "Popular Boosting Algorithms\n",
        "- AdaBoost: Adjusts weights of misclassified samples.\n",
        "- Gradient Boosting: Fits new models to the residual errors using gradient descent.\n",
        "- XGBoost: Adds regularization and optimization for speed and accuracy.\n",
        "- CatBoost: Handles categorical data efficiently.\n",
        "\n",
        "\n",
        "**Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?**\n",
        "* **AdaBoost trains models** by focusing on the mistakes made in previous rounds. It starts by training a simple model (like a shallow decision tree) on the entire dataset. After that, it looks at which data points were misclassified and increases their importance—so the next model pays more attention to those hard examples. This process continues for several rounds, with each new model trying to fix the errors of the previous one. At the end, all the models are combined, and the ones that performed better get more weight in the final decision.\n",
        "\n",
        "* **Gradient Boosting**, on the other hand, takes a more mathematical approach. Instead of changing the weights of the data points, it builds each new model to predict the errors (called residuals) made by the previous model. It uses a technique called gradient descent to minimize a loss function—basically, it tries to reduce the difference between the predicted and actual values step by step. Each model is added to the ensemble in a way that gradually improves the overall accuracy.\n",
        "\n",
        "So in short: AdaBoost adjusts the importance of data points based on mistakes, while Gradient Boosting builds models that directly learn from those mistakes using gradients. Both aim to improve performance, but they go about it in different ways\n",
        "\n",
        "\n",
        "**Question 3: How does regularization help in XGBoost?**\n",
        "*  Regularization in XGBoost refers to the addition of penalty terms to the objective function that discourage overly complex models. This is different from traditional gradient boosting, which typically lacks built-in regularization.\n",
        "XGBoost includes both L1 (Lasso) and L2 (Ridge) regularization:\n",
        "- L1 Regularization (alpha): Encourages sparsity in the model by pushing some leaf weights to zero. This can lead to simpler trees.\n",
        "- L2 Regularization (lambda): Penalizes large leaf weights, smoothing the model and reducing sensitivity to noise.\n",
        "\n",
        "**How It Helps-**\n",
        "- Reduces Overfitting: By penalizing complexity, regularization discourages the model from fitting noise in the training data.\n",
        "- Improves Generalization: Simpler models tend to perform better on new, unseen data.\n",
        "- Controls Tree Growth: Parameters like gamma (minimum loss reduction to make a split) and min_child_weight (minimum sum of instance weight in a child) act as structural regularizers.\n",
        "- Enhances Stability: Regularization makes the model less sensitive to small changes in the data, improving robustness.\n",
        "\n",
        "\n",
        "**Question 4: Why is CatBoost considered efficient for handling categorical data?**\n",
        "* **Native Support for Categorical Features-**\n",
        "Unlike most machine learning models that require manual preprocessing (like one-hot encoding or label encoding), CatBoost can natively process categorical variables. This saves time and reduces the risk of introducing bias or overfitting through improper encoding.\n",
        "\n",
        "* **Ordered Target Statistics -**\n",
        "CatBoost uses a technique called ordered boosting, which calculates target statistics (like mean target value per category) in a way that avoids target leakage. It does this by:\n",
        "- Generating multiple permutations of the dataset.\n",
        "- Computing statistics using only data points that come before the current one in each permutation.\n",
        "This ensures that the model doesn’t accidentally learn from future data, which would artificially inflate performance.\n",
        "\n",
        "* **Efficient Encoding of High-Cardinality -**\n",
        "CatBoost handles features with many unique categories (like ZIP codes or product IDs) without exploding the feature space. It uses efficient encoding strategies that preserve predictive power while keeping the model compact.\n",
        "\n",
        "* **Built-In Feature Combinations-**\n",
        "CatBoost automatically creates combinations of categorical features, which helps capture complex interactions between variables without manual feature engineering.\n",
        "\n",
        "* **Robust Performance-**\n",
        "Thanks to these innovations, CatBoost often delivers better accuracy and faster training on datasets with many categorical features, especially in domains like:\n",
        "- E-commerce (product categories)\n",
        "- Finance (transaction types)\n",
        "- Healthcare (diagnosis codes)\n",
        "\n",
        "In short, CatBoost’s smart handling of categorical data makes it a go-to choice for real-world problems where such features are common.\n",
        "\n",
        "\n",
        "**Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?**\n",
        "\n",
        "Boosting techniques are often preferred over bagging methods in real-world applications where high accuracy, error correction, and handling imbalanced or complex data are critical. Here are some key domains where boosting shines:\n",
        "\n",
        "1. **Fraud Detection-**\n",
        "Boosting algorithms like XGBoost and LightGBM are widely used in banking and e-commerce to detect fraudulent transactions. They excel at identifying rare patterns and anomalies in highly imbalanced datasets, where traditional bagging methods like Random Forest may struggle.\n",
        "\n",
        "2. **Medical Diagnosis-**\n",
        "In healthcare, boosting is used to predict diseases or patient outcomes based on complex clinical data. The ability to focus on hard-to-classify cases and reduce false negatives makes boosting ideal for sensitive applications like cancer detection or risk assessment.\n",
        "\n",
        "3. **Search Engine Ranking-**\n",
        "Boosting powers ranking algorithms such as LambdaMART, which is used in search engines and recommendation systems. These models learn to rank results based on user behavior and relevance, outperforming bagging methods in precision and ranking quality.\n",
        "\n",
        "4. **Credit Scoring and Risk Modeling-**\n",
        "Financial institutions use boosting to assess creditworthiness and predict loan defaults. Boosting models can capture subtle interactions between features like income, spending habits, and credit history, leading to more accurate risk predictions.\n",
        "\n",
        "5. **Customer Churn Prediction-**\n",
        "In telecom and subscription-based services, boosting helps identify customers likely to leave. It handles noisy and imbalanced data well, allowing businesses to take proactive retention measures.\n",
        "\n",
        "6. **Natural Language Processing (NLP)-**\n",
        "Boosting is used in text classification tasks such as spam detection, sentiment analysis, and topic modeling. Its ability to refine predictions over multiple iterations makes it effective for high-dimensional, sparse data like text.\n",
        "\n",
        " Why Boosting Over Bagging?\n",
        "- Boosting reduces bias and focuses on hard examples.\n",
        "- Bagging reduces variance and is better for noisy data.\n",
        "- When accuracy and interpretability are paramount, boosting is often the better choice.\n",
        "\n",
        "\n",
        "\n",
        "Datasets:\n",
        "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks.\n",
        "\n",
        "\n",
        "**Question 6: Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u59AvRunJmh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MdSA_KUXGKs",
        "outputId": "06f8aadb-c413-49e1-d2f3-dc3d646e2472"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZqMs87JEXNOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score**\n"
      ],
      "metadata": {
        "id": "YiHF37WtXNPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared Score: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RzTlRwQXLkk",
        "outputId": "40b312a8-0607-43ed-f542-f6f8faf39e72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy**\n"
      ],
      "metadata": {
        "id": "5G-lWad3Xx0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Define parameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUgEqp4yXLlu",
        "outputId": "f8bb4ad9-437a-4f4c-96f7-d470c3b4a4cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:53:51] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Test Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FbOsr2prY2HY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn**\n"
      ],
      "metadata": {
        "id": "sI4vOhxgY2Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train CatBoost Classifier\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "5eRP1AgsXGNO",
        "outputId": "d053e565-3e78-43dd-dab0-697b1138fd04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2441135988.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn**\n"
      ],
      "metadata": {
        "id": "Ob0TMd0Ybi2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-by-step pipeline using CatBoost for classification\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load dataset (simulating loan default classification)\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Simulate missing values\n",
        "X.iloc[::10, 0] = np.nan  # introduce missing values in first column\n",
        "\n",
        "# Simulate categorical feature\n",
        "X['region'] = np.random.choice(['North', 'South', 'East', 'West'], size=X.shape[0])\n",
        "cat_features = ['region']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CatBoostClassifier\n",
        "model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, cat_features=cat_features)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Default', 'Default'], yticklabels=['No Default', 'Default'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "b0HEUc3xMSbH",
        "outputId": "eb68e613-7843-4b68-8d1b-089e30552fbe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1795556451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load dataset (simulating loan default classification)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}